import streamlit as st
import feedparser
import trafilatura
import json
import os
import socket
import concurrent.futures
import base64
import requests
from openai import OpenAI
from duckduckgo_search import DDGS
from bs4 import BeautifulSoup
from fake_useragent import UserAgent

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø© Ø§Ù„Ø®Ø§ØµØ©
try:
    from manadger_lib import RSS_DATABASE, get_safe_key, ELITE_PROMPT
except ImportError:
    st.error("âŒ Ù…Ù„Ù manadger_lib.py Ù…ÙÙ‚ÙˆØ¯.")
    st.stop()

# ==========================================
# 0. Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ÙˆØ§Ù„ØªÙ‡ÙŠØ¦Ø©
# ==========================================
st.set_page_config(page_title="ÙŠÙ‚ÙŠÙ† Ø¨Ø±ÙŠØ³ | ØºØ±ÙØ© Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª", page_icon="ğŸ¦…", layout="wide")

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø£Ù…Ø§Ù† ÙˆØ§Ù„Ø´Ø¨ÙƒØ©
ua = UserAgent()
socket.setdefaulttimeout(30)

# ==========================================
# 1. Ø¯ÙˆØ§Ù„ Ø§Ù„Ù†Ø¸Ø§Ù… (Core Functions)
# ==========================================

# Ø¯Ø§Ù„Ø© Ø¬Ù„Ø¨ Ø§Ù„ØµÙˆØ± Ù…Ø¹ ØªØ­Ø³ÙŠÙ† Ø§Ù„ÙƒØ§Ø´ Ù„Ø¹Ø¯Ù… Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯
@st.cache_data(ttl=3600) # ÙŠØ­ÙØ¸ Ø§Ù„ØµÙˆØ± ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ù„Ù…Ø¯Ø© Ø³Ø§Ø¹Ø©
def get_base64_logo():
    if os.path.exists("logo.png"):
        with open("logo.png", "rb") as f:
            data = f.read()
        encoded = base64.b64encode(data).decode()
        return f'<img src="data:image/png;base64,{encoded}" style="width: 120px; display: block; margin: 0 auto;">'
    return ""

# Ù†Ø¸Ø§Ù… Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± (Ù…Ø­Ø±Ùƒ Ø§Ù„Ø±Ø§Ø¯Ø§Ø±) - Ù…Ø­Ø³Ù† Ù„Ù„Ø³Ø±Ø¹Ø©
@st.cache_data(ttl=900, show_spinner=False) # ØªØ­Ø¯ÙŠØ« ÙƒÙ„ 15 Ø¯Ù‚ÙŠÙ‚Ø© ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹
def fetch_news_category(category_name, sources):
    news_items = []
    
    def fetch_single_source(source_name, url):
        try:
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… User-Agent Ù„ØªØ¬Ù†Ø¨ Ø§Ù„Ø­Ø¸Ø±
            feed = feedparser.parse(url, agent=ua.random)
            if not feed.entries: return []
            
            return [{
                "title": entry.title,
                "link": entry.link,
                "source": source_name,
                "summary": getattr(entry, 'summary', '')[:200] + "..."
            } for entry in feed.entries[:6]] # Ù†ÙƒØªÙÙŠ Ø¨Ù€ 6 Ø£Ø®Ø¨Ø§Ø± Ø­Ø¯ÙŠØ«Ø© Ù„ÙƒÙ„ Ù…ØµØ¯Ø± Ù„Ù„Ø³Ø±Ø¹Ø©
        except:
            return []

    with concurrent.futures.ThreadPoolExecutor(max_workers=15) as executor:
        future_to_url = {executor.submit(fetch_single_source, name, url): name for name, url in sources.items()}
        for future in concurrent.futures.as_completed(future_to_url):
            data = future.result()
            if data: news_items.extend(data)
    
    return news_items

# Ù…Ø­Ø±Ùƒ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙˆØµ (Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ)
def process_article_with_ai(link, keyword):
    try:
        # 1. Ø³Ø­Ø¨ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø¨Ø°ÙƒØ§Ø¡
        downloaded = trafilatura.fetch_url(link)
        if not downloaded: return None, "ÙØ´Ù„ Ø³Ø­Ø¨ Ø§Ù„Ø±Ø§Ø¨Ø·"
        
        main_text = trafilatura.extract(downloaded, include_comments=False, include_tables=False)
        if not main_text or len(main_text) < 100: return None, "Ø§Ù„Ù…ØªÙˆÙ‰ Ù‚ØµÙŠØ± Ø¬Ø¯Ø§Ù‹ Ø£Ùˆ Ù…Ø­Ù…ÙŠ"

        # 2. ØªÙ†Ø¸ÙŠÙ Ø¥Ø¶Ø§ÙÙŠ
        soup = BeautifulSoup(main_text, "html.parser")
        clean_text = soup.get_text()[:4000] # Ù†Ø±Ø³Ù„ ÙÙ‚Ø· 4000 Ø­Ø±Ù Ù„ØªÙˆÙÙŠØ± Ø§Ù„ØªÙˆÙƒÙ†Ø²

        # 3. Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ù…Ø§Ù†Ø¯Ø¬Ø± AI
        api_key = get_safe_key()
        if not api_key: return None, "Ù…ÙØªØ§Ø­ API Ù…ÙÙ‚ÙˆØ¯"

        client = OpenAI(api_key=api_key, base_url="https://api.sambanova.ai/v1")
        
        response = client.chat.completions.create(
            model='Meta-Llama-3.3-70B-Instruct',
            messages=[
                {"role": "system", "content": "Ø£Ù†Øª Ù…Ø­Ø±Ø± ØµØ­ÙÙŠ Ù…Ø®Ø¶Ø±Ù… ÙÙŠ 'ÙŠÙ‚ÙŠÙ† Ø¨Ø±ÙŠØ³'. Ø§ÙƒØªØ¨ Ø¨Ø£Ø³Ù„ÙˆØ¨ Ø§Ø³ØªÙ‚ØµØ§Ø¦ÙŠ Ø±ØµÙŠÙ†."},
                {"role": "user", "content": ELITE_PROMPT.format(keyword=keyword) + f"\n\nØ§Ù„Ù†Øµ Ø§Ù„Ø£ØµÙ„ÙŠ:\n{clean_text}"}
            ],
            temperature=0.3
        )
        return response.choices[0].message.content, None
    except Exception as e:
        return None, str(e)

# ==========================================
# 2. Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø±Ø³ÙˆÙ…ÙŠØ© (UI)
# ==========================================

# CSS Ù…Ø®ØµØµ Ù„Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø¯Ø§ÙƒÙ† Ø§Ù„Ø§Ø­ØªØ±Ø§ÙÙŠ
st.markdown("""
<style>
    @import url('https://fonts.googleapis.com/css2?family=Tajawal:wght@400;700;900&display=swap');
    html, body, [class*="css"] { font-family: 'Tajawal', sans-serif; direction: rtl; }
    .stButton>button { width: 100%; border-radius: 8px; font-weight: bold; }
    .block-container { padding-top: 2rem; }
    h1, h2, h3 { color: #4aa3df !important; }
    .news-card { background-color: #262730; padding: 15px; border-radius: 10px; margin-bottom: 10px; border-right: 5px solid #4aa3df; }
</style>
""", unsafe_allow_html=True)

# Ø§Ù„Ù‡ÙŠØ¯Ø±
col_logo, col_title = st.columns([1, 4])
with col_logo:
    st.markdown(get_base64_logo(), unsafe_allow_html=True)
with col_title:
    st.title("Ù…Ù†ØµØ© ÙŠÙ‚ÙŠÙ† Ø¨Ø±ÙŠØ³ | YAQEEN PRESS")
    st.caption("Ù†Ø¸Ø§Ù… Ø§Ù„Ø³ÙŠØ§Ø¯Ø© Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§ØªÙŠØ© - Ù†Ø³Ø®Ø© Ø§Ù„Ø³Ø­Ø§Ø¨Ø© V2.0")

# Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙƒÙ„Ù…Ø© Ø§Ù„Ù…Ø±ÙˆØ± (ÙŠÙØ¶Ù„ Ù†Ù‚Ù„Ù‡Ø§ Ù„Ù€ st.secrets Ù„Ø§Ø­Ù‚Ø§Ù‹)
if "auth" not in st.session_state: st.session_state.auth = False

if not st.session_state.auth:
    pwd = st.text_input("ğŸ”‘ ÙƒÙˆØ¯ Ø§Ù„Ø¯Ø®ÙˆÙ„:", type="password")
    if st.button("Ø¯Ø®ÙˆÙ„"):
        if pwd == "Manager_Tech_2026": # ØºÙŠØ± Ù‡Ø°Ø§ Ù„Ø§Ø­Ù‚Ø§Ù‹!
            st.session_state.auth = True
            st.rerun()
        else:
            st.error("ÙƒÙˆØ¯ Ø®Ø§Ø·Ø¦")
    st.stop()

# ==========================================
# 3. Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª
# ==========================================

# Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠØ© (Sidebar) Ù„Ù„ØªØ­ÙƒÙ…
with st.sidebar:
    st.header("ğŸ® ÙˆØ­Ø¯Ø© Ø§Ù„ØªØ­ÙƒÙ…")
    selected_category = st.selectbox("Ø§Ø®ØªØ§Ø± Ø§Ù„Ù‚Ø·Ø§Ø¹:", list(RSS_DATABASE.keys()))
    
    st.divider()
    keyword_input = st.text_input("Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© (SEO):", "ÙŠÙ‚ÙŠÙ† Ø¨Ø±ÙŠØ³")
    
    if st.button("Ù…Ø³Ø­ Ø§Ù„ÙƒØ§Ø´ (ØªØ­Ø¯ÙŠØ« Ø¥Ø¬Ø¨Ø§Ø±ÙŠ)"):
        st.cache_data.clear()
        st.rerun()

# Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±
with st.spinner(f"Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ø£Ù‚Ù…Ø§Ø± Ø§Ù„ØµÙ†Ø§Ø¹ÙŠØ© Ù„Ø¬Ù„Ø¨ Ø£Ø®Ø¨Ø§Ø± {selected_category}..."):
    news_list = fetch_news_category(selected_category, RSS_DATABASE[selected_category])

if not news_list:
    st.warning("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£Ø®Ø¨Ø§Ø±ØŒ Ø£Ùˆ Ù‡Ù†Ø§Ùƒ Ù…Ø´ÙƒÙ„Ø© ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ù…ØµØ§Ø¯Ø±.")
    st.stop()

st.success(f"ØªÙ… Ø±ØµØ¯ {len(news_list)} Ø®Ø¨Ø±Ø§Ù‹ Ø³Ø§Ø®Ù†Ø§Ù‹ ğŸ”¥")

# Ø¹Ø±Ø¶ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± ÙˆØ§Ø®ØªÙŠØ§Ø± Ø£Ø­Ø¯Ù‡Ø§
# Ù†Ù‚ÙˆÙ… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¦Ù…Ø© Ù„Ù„Ø¹Ø±Ø¶ ÙÙ‚Ø·
display_options = [f"{item['source']} - {item['title']}" for item in news_list]
selected_index = st.selectbox("Ø§Ø®ØªØ± Ø§Ù„Ø®Ø¨Ø± Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©:", range(len(news_list)), format_func=lambda x: display_options[x])

target_news = news_list[selected_index]

# Ø²Ø± Ø§Ù„ØªÙ†ÙÙŠØ°
if st.button(f"ğŸš€ ØµÙŠØ§ØºØ© Ø§Ù„Ø®Ø¨Ø±: {target_news['title'][:30]}..."):
    st.info(f"Ø§Ù„Ù…ØµØ¯Ø±: {target_news['source']} | Ø¬Ø§Ø±ÙŠ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©...")
    
    article_content, error = process_article_with_ai(target_news['link'], keyword_input)
    
    if error:
        st.error(f"Ø­Ø¯Ø« Ø®Ø·Ø£: {error}")
    else:
        st.balloons()
        st.markdown("### âœ¨ Ø§Ù„Ù…Ù‚Ø§Ù„ Ø§Ù„Ø¬Ø§Ù‡Ø² Ù„Ù„Ù†Ø´Ø±")
        
        # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ø¹Ù† Ø§Ù„Ù…Ø­ØªÙˆÙ‰ (Ø§ÙØªØ±Ø§Ø¶ Ø£Ù† Ø§Ù„Ø³Ø·Ø± Ø§Ù„Ø£ÙˆÙ„ Ø¹Ù†ÙˆØ§Ù†)
        lines = article_content.split('\n')
        title = lines[0].replace('Ø§Ù„Ø¹Ù†ÙˆØ§Ù†:', '').strip()
        body = '\n'.join(lines[1:])
        
        # Ø¹Ø±Ø¶ Ù…Ù†Ø³Ù‚
        st.text_input("Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ù…Ù‚ØªØ±Ø­:", value=title)
        st.text_area("Ù†Øµ Ø§Ù„Ù…Ù‚Ø§Ù„ (Ø¬Ø§Ù‡Ø² Ù„Ù„Ù†Ø³Ø®):", value=body, height=400)
        
        st.markdown("---")
        st.markdown(f"**Ø±Ø§Ø¨Ø· Ø§Ù„Ù…ØµØ¯Ø±:** [Ø§Ø¶ØºØ· Ù‡Ù†Ø§]({target_news['link']})")
